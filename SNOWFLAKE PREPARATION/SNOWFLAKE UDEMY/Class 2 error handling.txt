*************** COPY OPTION: ON_ERROR *************

CREATE OR REPLACE STAGE MANAGE_DB.external_stages.aws_stage_errorex
	url='s3://nucketsnowflakes4';

//List fiels in stage
LIST @MANAGE_DB.external_stages.aws_stage_errorex;

//Create example table
CREATE OR REPLACE TABLE OUR_FIRST_DB.PUBLIC.ORDERS_EX (
	ORDER_ID VARCHAR(30),
	AMOUNT INT,
	PROFIT INT,
	QUANTITY INT,
	CATEGORY VARCHAR(30),
	SUBCATEGORY VARCHAR(30));

//Demonstrating error message
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
	FROM @MANAGE_DB.external_stages.aws_stage_errorex
	file_format= (type=csv field_delimiter=',' skip_header=1)
	files = ('OrderDetails_error.csv');

	//Error handling using the ON_ERROR option

COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
	FROM @MANAGE_DB.external_stages.aws_stage_errorex
	file_format= (type=csv field_delimiter=',' skip_header=1)
	files = ('OrderDetails_error.csv')
	ON_ERROR = 'CONTINUE'; --> This will load the data that comes after the errors or that are not not errors anyways. Ignores the previous errors.

TRUNCATE TABLE FOR OUR_FIRST_DB.PUBLIC.ORDERS_EX;

// Error handling using the ON_ERROR option = ABORT_STATEMENT (default)
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
	FROM @MANAGE_DB.external_stages.aws_stage_errorex
	file_format= (type = csv field_delimiter=',' skip_header=1)
	files = ('OrderDetails_error.csv','OrderDetails_error2.csv')
	ON_ERROR = 'ABORT_STATEMENT'; --> **** This is the default value *****


// Error handling using the ON_ERROR option = SKIP_FILE (default)
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
	FROM @MANAGE_DB.external_stages.aws_stage_errorex
	file_format= (type = csv field_delimiter=',' skip_header=1)
	files = ('OrderDetails_error.csv','OrderDetails_error2.csv')
	ON_ERROR = 'SKIP_FILE';

// Error handling using the ON_ERROR option = SKIP_FILE_<number>, <number> here is the error limitation after exceeding this you will not load the data!!

COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
	FROM @MANAGE_DB.external_stages.aws_stage_errorex
	file_format= (type = csv field_delimiter=',' skip_header=1)
	files = ('OrderDetails_error.csv','OrderDetails_error2.csv')
	ON_ERROR = 'SKIP_FILE_2';---> _2 is the limit that we accept error records


			OR
COPY INTO OUR_FIRST_DB.PUBLIC.ORDERS_EX
	FROM @MANAGE_DB.external_stages.aws_stage_errorex
	file_format= (type = csv field_delimiter=',' skip_header=1)
	files = ('OrderDetails_error.csv','OrderDetails_error2.csv')
	ON_ERROR = 'SKIP_FILE_3%';---> PERCENTAGE OF ERRORS it can accept is the limit that we accept error records


// creating file format object
CREATE OR REPLACE FILE FORMAT db.schema.my_file_format;

// see properties of file format object
DESC file format db.schema.my_file_format;

// Using file format object in copy command

COPY INTO OUR DB.PUBLIC.ORDERS_EX
	FROM @MANAGE_DB.schemaname.stagenameXYZ
	file_format= (FORMAT_NAME=MANAGE_DB.file_formats.my_file_format)
	files = ('OrderDetails_error.csv')
	ON_ERROR = 'SKIP_FILE_3';

ALTER FILE FORMAT MANAGE_DB.file_formats.my_file_format
	SET SKIP_HEADER = 1;

CREATE OR REPLACE FILE FORMAT MANAGE_DB.
	TYPE=JSON,
	TIME_FORMAT=AUTO;
***Note: We can only create or replace file_format objects but not alter or replace them! :)

//Overwriting properties of file format object
COPY INTO OUR DB.PUBLIC.ORDERS_EX
	FROM @MANAGE_DB.schemaname.stagenameXYZ
	file_format= (FORMAT_NAME=MANAGE_DB.file_formats.my_file_format field_delimiter = ',' skip_header=1)
	files = ('OrderDetails_error.csv')
	ON_ERROR = 'SKIP_FILE_3';

		
		$$$$$$$$$$4 VALIDATION_MODE $$$$$$$$$$$$$$

//If there are any errors then it will return all after validation
COPY INTO COPY_DB.PUBLIC.ORDERS
	FROM @aws_stage_copy
	file_format= (type=csv field_delimiter=',' skip_header=1)
	pattern='.*Order.*'
	VALIDATION_MODE= RETURN_ERRORS;

 // Returns only the mentioned number of rows that are validated
COPY INTO COPY_DB.PUBLIC.ORDERS
	FROM @aws_stage_copy
	file_format= (type=csv field_delimiter=',' skip_header=1)
	pattern='.*Order.*'
	VALIDATION_MODE = RETURN_5_ROWS;
	

&&&&&&&&&&&&&& Working with rejected records &&&&&&&&&&&&&&&&&&&&&&&&&&&&&

// Storing rejected/failed results in a table
CREATE OR REPLACE TABLE rejected AS
select rejected_record from table(result_scan(last_query_id()));

- result_scan will have results within the last 24 hours!!
- last_query_id() will have the last query id

-- Adding additional records ---
INSERT INTO rejected
select rejected_record from table(result_scan(last_query_id()));

--------- 2) Saving rejected files without VALIDATION_MODE -----

COPY INTO COPY_DB.PUBLIC.ORDERS
	FROM @aws_stage_copy
	file_format= (type=csv field_delimiter=',' skip_header=1)
	ON_ERROR=CONTINUE;

select * from table(validate(orders, job_id =>'_last'));

- '_last' contains the last copy command executed. We can also use query_id in place of this.
- validate gives the last 

--------- 3) Working with rejected records --------------

SELECT REJECTED_RECORD FROM rejected; --> This rejected record contains complete record with all the columns data in it. Its data type is called variant!!

CREATE OR REPLACE TABLE rejected_values as
SELECT
SPLIT_PART(rejected_record,',',1) AS ORDER_ID,
SPLIT_PART(rejected_record,',',2) AS AMOUNT,
SPLIT_PART(rejected_record,',',3) AS PROFIT,
SPLIT_PART(rejected_record,',',4) AS QUANTITY,
SPLIT_PART(rejected_record,',',5) AS CATEGORY,
SPLIT_PART(rejected_record,',',6) AS SUBCATEGORY,
FROM rejected;

SELECT * FROM rejected_values;

************** SIZE_LIMIT *************

This size limit is in "BYTES" after reaching its threshold we need to stop loading.....
Even if we have multiple files, if the limit is reached within the first file then we need to stop at that particular record itself!!!!!

Example:
COPY INTO COPY_DB.PUBLIC.ORDERS
	FROM @aws_stage_copy
	file_format=(type= csv field_delimiter=',' skip_header=1)
	pattern='.*Order.*'
	SIZE_LIMIT=60000;

Question 3:
We have 3 files in our external stage with 20MB (~20 000 000 bytes) each.
How many files will be loaded if we set SIZE_LIMIT = 30 000 000 ?
Answer : 2 files

That is correct! The first one will be always loaded. After that only 20MB have been loaded and the second file will be loaded as well. After that we have loaded 40MB and the size limit has been exceeded - therefore no more file will be loaded.

********* RETURN_FAILED_ONLY ******

Specifies whether to return only the FILES that have failed to load into the statement result.

Note: Mandatory to use ON_ERROR=CONTINUE command becoz without that we cannot load any data. This will skip the files with errors, so that we can use this RETURN_FAILED_ONLY command!!!

COPY INTO COPY_DB.PUBLIC.ORDERS
	FROM @aws_stage_copy
	file_format= (type=csv field_delimiter=',' skip_header=1)
	pattern='.*Order.*'
	ON_ERROR = CONTINUE
	RETURN_FAILED_ONLY = TRUE;

************** TRUNCATECOLUMNS *******************************

Specifies whether to truncate text strings that exceed the target column length
TRUE = strings are automatically truncated to the target column length
FALSE = COPY produces an error if a loaded string exceeds the target column length
DEFAULT = FALSE

COPY INTO <TABLE_NAME>
FROM externalStage
FILES = ('<file_name>','<file_name2>')
FILE_FORMAT = <file_format_name>
TRUNCATECOLUMNS = TRUE | FALSE

Example: If source has a column CATEGORY with varchar of 12 characters but the target table is only accepting a varchar of 10 characters then we use this "truncatecolumns" option to accept only this 10 characters of data! Instead of throwing ERRORS!! :):):)

*********** FORCE OPTION ***********
Generally snowflake will not use LOAD/COPY INTO if it is already executed because it will create a duplicate data. But by using FORCE we can force to rerun the copy into command!!!

COPY INTO <TABLE_NAME>
FROM externalStage
FILES = ('<file_name>','<file_name2>')
FILE_FORMAT = <file_format_name>
FORCE = TRUE | FALSE

******************* LOAD HISTORY *****************************
Enables you to retrieve the history of data loaded into tables using the COPY INTO <table> command

snowflake db is a common snowflake database or global database!! 

Db: Snowflake
schemaname: account_usage
tablename: load_history

This table load_history contains the filenames, load date&times,status of pratial or failed or loaded, no.of rows parsed as row_parsed, first_error_message, first_error_col_name, errorCount, errorLimit and so on.

// Filter on specific table & schema
select * from snowflake.account_usage.load_history
where schema_name='public' and table_name='orders';

2) filter on specific table & schema with zero error_count!!!
select * from snowflake.account_usage.load_history
where schema_name='public' and table_name='orders' and
error_count >0;

3) select * from snowflake.account_usage.load_history
where DATE(LAST_LOAD_TIME) <= DATEADD(days,-1,CURRENT_DATE);
































































































